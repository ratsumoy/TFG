{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GENERAL ANALYSIS"
      ],
      "metadata": {
        "id": "Kd-tItXC3O29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pysentimiento\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mM4aeQgS8rvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJdJAHb5fCnE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "from pysentimiento import create_analyzer\n",
        "from google.colab import drive\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MO-GwAQGRXQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "hate_analyzer = create_analyzer(task=\"hate_speech\", lang=\"es\")"
      ],
      "metadata": {
        "id": "DOPeZBsxvuch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_text(text_series, analyzer):\n",
        "    results = text_series.map(lambda x: analyzer.predict(x))\n",
        "    labels, scores = zip(*[(r.output, r.probas) for r in results])\n",
        "    return labels, scores\n",
        "\n",
        "# Mean probability of each hate label\n",
        "def analyze_hate_probabilities(text_series, analyzer):\n",
        "    prob_sums = defaultdict(float)\n",
        "    count = 0\n",
        "\n",
        "    for text in text_series:\n",
        "        result = analyzer.predict(text)\n",
        "        for label, prob in result.probas.items():\n",
        "            prob_sums[label] += prob\n",
        "        count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        return None\n",
        "\n",
        "    avg_probs = {label: prob_sum / count for label, prob_sum in prob_sums.items()}\n",
        "    return avg_probs\n",
        "\n",
        "\n",
        "# Function to analyse sentiment score\n",
        "def process_folder_sentiment(party_path):\n",
        "    sentiment_scores = []\n",
        "    for file_name in os.listdir(party_path):\n",
        "        if not file_name.endswith(\".csv\"):\n",
        "            continue\n",
        "        file_path = os.path.join(party_path, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, delimiter='\\t')\n",
        "        except pd.errors.EmptyDataError:\n",
        "            continue\n",
        "\n",
        "        if 'text' not in df.columns or 'id' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        df = df[df['text'].notna() & (df['text'] != '')]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "\n",
        "        labels, scores = analyze_sentiment_text(df['text'], sentiment_analyzer)\n",
        "        df['sentiment_label'] = labels\n",
        "        sentiment_mapping = {'POS': 1, 'NEU': 0, 'NEG': -1}\n",
        "        df['sentiment_score'] = df['sentiment_label'].map(sentiment_mapping)\n",
        "\n",
        "        sentiment_scores.extend(df['sentiment_score'].tolist())\n",
        "\n",
        "    if len(sentiment_scores) == 0:\n",
        "        print(\"No sentiment scores computed for this party.\")\n",
        "        return None\n",
        "\n",
        "    avg_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    print(f\"Average sentiment score for party at {party_path}: {avg_score:.3f}\")\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "# Function to analyse hate speech\n",
        "def process_party_hate_avg(party_path, analyzer):\n",
        "    all_texts = []\n",
        "\n",
        "    for filename in os.listdir(party_path): # Iterates over each csv file\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "        filepath = os.path.join(party_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, delimiter=\"\\t\", on_bad_lines='skip')\n",
        "        except pd.errors.EmptyDataError:\n",
        "            continue\n",
        "\n",
        "        if 'text' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        texts = df['text'].dropna().astype(str)\n",
        "        texts = texts[texts != '']\n",
        "\n",
        "        all_texts.extend(texts.tolist())\n",
        "\n",
        "    if not all_texts:\n",
        "        return None\n",
        "\n",
        "    return analyze_hate_probabilities(all_texts, analyzer)\n",
        "\n"
      ],
      "metadata": {
        "id": "xUz57g9rvxCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "party_path = 'PARTY_FOLDER'"
      ],
      "metadata": {
        "id": "7TkhjeJeweb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_folder_sentiment(party_path)"
      ],
      "metadata": {
        "id": "fyXCsU5BwiW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_party_hate_avg(party_path, hate_analyzer)"
      ],
      "metadata": {
        "id": "R7iQwsvtwkOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CASE STUDY ANALYSIS"
      ],
      "metadata": {
        "id": "-mqsGeO0w8OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join all comments\n",
        "all_comments = []\n",
        "\n",
        "for file in os.listdir(party_path):\n",
        "    if file.startswith(\"comments_\") and file.endswith(\".csv\"):\n",
        "        file_path = os.path.join(party_path, file)\n",
        "        df = pd.read_csv(file_path, delimiter='\\t', encoding='utf-8')\n",
        "        df['file'] = file\n",
        "\n",
        "        all_comments.append(df)\n",
        "\n",
        "full_comments = pd.concat(all_comments, ignore_index=True)\n",
        "full_comments.head()"
      ],
      "metadata": {
        "id": "m8lV9djrjORd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_comments_sorted = full_comments.sort_values(['like_count'], ascending = False)"
      ],
      "metadata": {
        "id": "xmBvvtAmVEXf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_comments_sorted = full_comments_sorted[full_comments_sorted['text'].notna()]\n",
        "\n",
        "# Sentiment analyser\n",
        "sentiment_labels, _ = analyze_sentiment_text(full_comments_sorted['text'], sentiment_analyzer)\n",
        "full_comments_sorted['sentiment'] = sentiment_labels\n",
        "sentiment_mapping = {'POS': 1, 'NEU': 0, 'NEG': -1}\n",
        "full_comments_sorted['sentiment_score'] = full_comments_sorted['sentiment'].map(sentiment_mapping)"
      ],
      "metadata": {
        "id": "Wak9Tw2afa6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hate analyser\n",
        "full_comments_sorted['hate_label'] = full_comments_sorted['text'].map(lambda x: hate_analyzer.predict(x).probas)"
      ],
      "metadata": {
        "id": "508V-PcT8RVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 30 comments\n",
        "full_comments_sorted[['text', 'video_id', 'sentiment']].head(30)"
      ],
      "metadata": {
        "id": "yM8KBX_08Eth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIND VIDEO WITH MOST LIKED COMMENT"
      ],
      "metadata": {
        "id": "Vy2uvosGXETO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_video_file(video_id, voice_path, party):\n",
        "\n",
        "    party_folder = os.path.join(voice_path, party)\n",
        "\n",
        "    for csv_file in os.listdir(party_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            file_path = os.path.join(party_folder, csv_file)\n",
        "            try:\n",
        "                df = pd.read_csv(file_path, delimiter=\"\\t\")\n",
        "                if 'id' in df.columns and video_id in df['id'].astype(str).values:\n",
        "                    return csv_file  # Returns csv file, and now it is easy to find the video to get the content\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading {file_path}: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "Qg1nRib5Vp5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "voice_path = \"PARTY_PATH\"\n",
        "video_id = 'VIDEO_ID'\n",
        "party = 'PARTY NAME FILE'\n",
        "find_video_file(video_id, voice_path, party)"
      ],
      "metadata": {
        "id": "ft8Kr_IjYE88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse comment text\n",
        "text = 'es que yo creo que Alberto Núñez fejo sigue cometiendo el mismo error qué error el error es eh primero no comprender el Marco constitucional ni la democracia primero Alberto Núñez fejo sigue um sin comprender que hay 1 artículo 99 en la Constitución española que dice que el modelo constitucional es 1 1 modelo parlamentario en el que 1 tiene que buscar apoyos 2 tiene disfunciones con la democracia habla de partido de estado el partido que está bloqueando 1 de las instituciones fundamentales del estado se llama consejo general del poder judicial 1 partido que instrumentaliza todas y cada 1 de las instituciones que toca si se aplica el término de estado entiendo que lo dice de manera institucional que cumpla con el mandato y renueve los órganos constitucionales pendientes sería bueno que lo haga así y que sea 1 demócrata y 1 demócrata sabe que vamos a IR a 1 investidura en la que él no tiene aliados y no tiene votos es 1 investidura absolutamente fracasada ya sé que el corsé de la democracia y de la institucionalidad a Alberto 1 espejo le queda grande para muestra 1 botón el consejo general del poder judicial que no es pequeña cosa'\n",
        "sentiment_result = sentiment_analyzer.predict(text)\n",
        "print(sentiment_result)\n"
      ],
      "metadata": {
        "id": "82ZAqv3xB0Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cphULFbhCGfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA"
      ],
      "metadata": {
        "id": "d3huNx19bt-R"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j3C2Qjb6dn-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}