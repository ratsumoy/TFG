{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejZU8lljvwrW",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install pysentimiento"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from pysentimiento import create_analyzer\n",
        "from google.colab import drive\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "MIlboTB_vRar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hO_RWoYuwBPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GENERAL ANALYSIS"
      ],
      "metadata": {
        "id": "7duYz1nSha2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load sentiment and hate-speech analyzers\n",
        "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
        "hate_analyzer = create_analyzer(task=\"hate_speech\", lang=\"es\")"
      ],
      "metadata": {
        "id": "o73mlF7x6k7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentiment_text(text_series, analyzer):\n",
        "    results = text_series.map(lambda x: analyzer.predict(x))\n",
        "    labels, scores = zip(*[(r.output, r.probas) for r in results])\n",
        "    return labels, scores\n",
        "\n",
        "# Mean probability of each hate label\n",
        "def analyze_hate_probabilities(text_series, analyzer):\n",
        "    # Sum for each label, we need a dictionary since the output is a dict of three labels and their probability (for each text)\n",
        "    prob_sums = defaultdict(float)\n",
        "    count = 0\n",
        "\n",
        "    for text in text_series:\n",
        "        result = analyzer.predict(text)\n",
        "        for label, prob in result.probas.items():\n",
        "            prob_sums[label] += prob\n",
        "        count += 1\n",
        "\n",
        "    if count == 0:\n",
        "        return None\n",
        "\n",
        "    avg_probs = {label: prob_sum / count for label, prob_sum in prob_sums.items()}\n",
        "    return avg_probs\n",
        "\n",
        "\n",
        "# Sentiment score\n",
        "def process_folder_sentiment(party_path):\n",
        "    sentiment_scores = []\n",
        "    for file_name in os.listdir(party_path):\n",
        "        if not file_name.endswith(\".csv\"):\n",
        "            continue\n",
        "        file_path = os.path.join(party_path, file_name)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, delimiter='\\t')\n",
        "        except pd.errors.EmptyDataError:\n",
        "            continue\n",
        "\n",
        "        if 'video_description' not in df.columns or 'id' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        df = df[df['video_description'].notna() & (df['video_description'] != '')]\n",
        "        if df.empty:\n",
        "            continue\n",
        "\n",
        "\n",
        "        labels, _ = analyze_sentiment_text(df['video_description'], sentiment_analyzer)\n",
        "        df['sentiment_label'] = labels\n",
        "        score_map = {'POS': 1, 'NEU': 0, 'NEG': -1}\n",
        "        df['sentiment_score'] = df['sentiment_label'].map(score_map)\n",
        "\n",
        "        sentiment_scores.extend(df['sentiment_score'].tolist())\n",
        "\n",
        "    if len(sentiment_scores) == 0:\n",
        "        print(\"No sentiment scores computed for this party.\")\n",
        "        return None\n",
        "\n",
        "    avg_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "    print(f\"Average sentiment score for party at {party_path}: {avg_score:.3f}\")\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "# Function to analyse hate speech\n",
        "def process_party_hate_avg(party_path, analyzer):\n",
        "    all_texts = []\n",
        "\n",
        "    for filename in os.listdir(party_path): # Iterates over each csv file\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "        filepath = os.path.join(party_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, delimiter=\"\\t\", on_bad_lines='skip')\n",
        "        except pd.errors.EmptyDataError:\n",
        "            continue\n",
        "\n",
        "        # Do the same for voice_to_text\n",
        "        if 'video_description' not in df.columns:\n",
        "            continue\n",
        "\n",
        "        texts = df['video_description'].dropna().astype(str)\n",
        "        texts = texts[texts != '']\n",
        "\n",
        "        all_texts.extend(texts.tolist())\n",
        "\n",
        "    if not all_texts:\n",
        "        return None\n",
        "\n",
        "    # Analyse and return the probabilities for each label\n",
        "    return analyze_hate_probabilities(all_texts, analyzer)\n",
        "\n"
      ],
      "metadata": {
        "id": "m-TZx32pxmjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the Party's folder\n",
        "party_path = 'PARTY_FOLDER'"
      ],
      "metadata": {
        "id": "SnkD90dJoDbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_folder_sentiment(party_path)"
      ],
      "metadata": {
        "id": "Ec0NL3OyVxSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_party_hate_avg(party_path, hate_analyzer)"
      ],
      "metadata": {
        "id": "epFM71HKuriY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOTAL NUM OF VIDEOS PER PARTY"
      ],
      "metadata": {
        "id": "WtrxaE3ttBSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = 'CSV_PATH' # Path where the files are saved\n",
        "\n",
        "video_counts = {}\n",
        "\n",
        "for party in os.listdir(base_path):\n",
        "    party_path = os.path.join(base_path, party)\n",
        "    if os.path.isdir(party_path):\n",
        "        total_lines = 0\n",
        "        for file_name in os.listdir(party_path):\n",
        "            if file_name.endswith('.csv'):\n",
        "                file_path = os.path.join(party_path, file_name)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, on_bad_lines='skip')\n",
        "                    total_lines += len(df)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "        video_counts[party] = total_lines\n",
        "\n",
        "video_counts_df = pd.DataFrame(list(video_counts.items()), columns=['Party', 'Total num videos'])\n",
        "video_counts_df = video_counts_df.sort_values(by='Total num videos', ascending=False).reset_index(drop=True)\n",
        "\n",
        "print(video_counts_df)"
      ],
      "metadata": {
        "id": "MOjkmoPqtAAp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}